import streamlit as st
import os
import json
import toml
from reader.reader import DocumentReader
from RAG.faiss_indexer import FaissIndexer
from generator.response_generator import ResponseGenerator
from generator.chat_history_control import ControlChatHistoryData
from naive_bayes_model.naive_bayes_classifier import NaiveBayesClassifier
from langchain_ollama import ChatOllama
import re

# è¯»å–é…ç½®æ–‡ä»¶
config = toml.load("config/parameter.toml")
bert_uncased_model_name = config["bert"]["model_name"]

# æ¨¡å‹Aå‚æ•°
base_url_A = config["model_A"]["base_url"]
local_model_name_A = config["model_A"]["model_name"]
temperature_A = config["model_A"]["temperature"]
top_p_A = config["model_A"]["top_p"]
top_k_A = config["model_A"]["top_k"]

# åˆå§‹åŒ–LangChainæœ¬åœ°æ¨¡å‹éƒ¨ç½²
model_A = ChatOllama(
    base_url=base_url_A,
    model=local_model_name_A,
    temperature=temperature_A,
    top_p=top_p_A,
    top_k=top_k_A
)

st.set_page_config(page_title="ç½‘å®‰æ™ºè„‘ - AI", page_icon="web/favicon.png")
# åˆå§‹åŒ–å¯¹è¯å†å²è®°å½•æ§åˆ¶ç³»ç»Ÿ
history_control = ControlChatHistoryData()

# åˆå§‹åŒ–æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
classifier = NaiveBayesClassifier()
model_path = "./naive_bayes_model/saved_model/naive_bayes_model.pkl"
vectorizer_path = "./naive_bayes_model/saved_vectorizer/naive_bayes_vectorizer.pkl"
train_data_path = "./naive_bayes_model/train_data/train_data"

# åŠ è½½æˆ–è®­ç»ƒæ¨¡å‹
if not os.path.exists(model_path) or not os.path.exists(vectorizer_path):
    classifier.train(train_data_path)
    classifier.save_model(model_path=model_path, vectorizer_path=vectorizer_path)
else:
    classifier.load_model(model_path, vectorizer_path)

# åˆå§‹åŒ–å›ç­”å™¨
response_generator = ResponseGenerator(model_A)

# åˆå§‹åŒ–ç´¢å¼•å™¨
faiss_indexer_net = FaissIndexer()
faiss_indexer_laws = FaissIndexer()
faiss_indexer_cases = FaissIndexer()

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
if not os.path.exists("./logs"):
    os.mkdir("logs")

def save_last_user_id(user_id):
    with open("logs/last_user_id.txt", "w") as f:
        f.write(user_id)

def load_last_user_id():
    if os.path.exists("logs/last_user_id.txt"):
        with open("logs/last_user_id.txt", "r") as f:
            return f.read().strip()
    return ""

def save_last_library_paths(net_path, law_path, case_path):
    data = {
        "net_path": net_path,
        "law_path": law_path,
        "case_path": case_path
    }
    with open("logs/library_paths.json", "w") as f:
        json.dump(data, f)

def load_last_library_paths():
    if os.path.exists("logs/library_paths.json"):
        with open("logs/library_paths.json", "r") as f:
            return json.load(f)
    return {"net_path": "", "law_path": "", "case_path": ""}

def process_library_folder(folder_path, library_name):
    document_reader = DocumentReader()
    paragraphs = []
    base_folder = "docs"
    output_folder = os.path.join(base_folder, library_name)
    output_file_path = os.path.join(output_folder, f"{library_name}_rag_data.txt")

    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    if os.path.exists(output_file_path):
        os.remove(output_file_path)

    for root, dirs, files in os.walk(folder_path):
        for file in files:
            file_extension = os.path.splitext(file)[1].lower()
            if file_extension in [".pdf", ".docx", ".txt"]:
                file_path = os.path.join(root, file)
                if file_path != output_file_path:
                    text = document_reader.read_file(file_path)
                    text = text.replace("\n", "  ")
                    formatted_text = f"{file}: {text}"
                    paragraphs.append(formatted_text)

    with open(output_file_path, 'w', encoding='utf-8') as f:
        for paragraph in paragraphs:
            f.write(paragraph + "\n\n")

    return paragraphs

# åˆå§‹åŒ–ä¾§è¾¹æ 
st.sidebar.title("ç”¨æˆ·å’Œä¼šè¯ç®¡ç†")

# ç”¨æˆ·IDç®¡ç†
last_user_id = load_last_user_id()
user_id = st.sidebar.selectbox("é€‰æ‹©ç”¨æˆ·ID", ["æ–°å»ºç”¨æˆ·"] + history_control.list_user_ids(), index=0 if not last_user_id else history_control.list_user_ids().index(last_user_id) + 1)

if user_id == "æ–°å»ºç”¨æˆ·":
    new_user_id = st.sidebar.text_input("è¯·è¾“å…¥æ–°çš„ç”¨æˆ·ID")
    if st.sidebar.button("åˆ›å»ºç”¨æˆ·"):
        if new_user_id and new_user_id not in history_control.list_user_ids():
            history_control.create_new_user(new_user_id)
            user_id = new_user_id
            save_last_user_id(user_id)
            st.sidebar.success(f"ç”¨æˆ· {user_id} å·²åˆ›å»º")
        else:
            st.sidebar.error("ç”¨æˆ·IDå·²å­˜åœ¨æˆ–æ— æ•ˆ")
else:
    save_last_user_id(user_id)

# åˆ é™¤ç”¨æˆ·
if st.sidebar.button("åˆ é™¤å½“å‰ç”¨æˆ·"):
    if user_id and user_id != "æ–°å»ºç”¨æˆ·":
        history_control.delete_user_history(user_id)
        st.sidebar.success(f"ç”¨æˆ· {user_id} çš„æ‰€æœ‰ä¼šè¯å·²åˆ é™¤")
        user_id = "æ–°å»ºç”¨æˆ·"  # é‡ç½®ç”¨æˆ·ID
        save_last_user_id(user_id)  # ä¿å­˜æœ€æ–°çš„ç”¨æˆ·ID

# ä¼šè¯ç®¡ç†
selected_session = st.sidebar.selectbox("é€‰æ‹©ä¼šè¯", ["æ–°å»ºä¼šè¯"] + history_control.list_session_ids(user_id))

if selected_session == "æ–°å»ºä¼šè¯":
    session_id = st.sidebar.text_input("æ–°å»ºä¼šè¯ ID", key="new_session_id")
    if st.sidebar.button("åˆ›å»ºä¼šè¯"):
        history_control.create_new_session(user_id, session_id)
        st.sidebar.success(f"ä¼šè¯ {session_id} å·²åˆ›å»º")
        selected_session = session_id
else:
    session_id = selected_session

# åˆ é™¤ä¼šè¯
if st.sidebar.button("åˆ é™¤å½“å‰ä¼šè¯"):
    if session_id and session_id != "æ–°å»ºä¼šè¯":
        history_control.delete_session_history(user_id, session_id)
        st.sidebar.success(f"ä¼šè¯ {session_id} å·²åˆ é™¤")
        selected_session = "æ–°å»ºä¼šè¯"  # é‡ç½®ä¼šè¯ID
        st.session_state.chat_history = []  # æ¸…é™¤å‰ç«¯èŠå¤©è®°å½•

# æ–‡æ¡£è·¯å¾„è¾“å…¥
last_paths = load_last_library_paths()
net_file_path = st.sidebar.text_input("ç½‘å®‰çŸ¥è¯†åº“è·¯å¾„", value=last_paths["net_path"])
law_file_path = st.sidebar.text_input("æ³•å¾‹æ³•è§„åº“è·¯å¾„", value=last_paths["law_path"])
case_file_path = st.sidebar.text_input("æ¡ˆä¾‹åº“è·¯å¾„", value=last_paths["case_path"])

# ä¿å­˜å½“å‰æ–‡æ¡£è·¯å¾„
save_last_library_paths(net_file_path, law_file_path, case_file_path)

# å¤„ç†åº“æ–‡ä»¶å¤¹
net_paragraphs = process_library_folder(net_file_path, "net")
law_paragraphs = process_library_folder(law_file_path, "laws")
case_paragraphs = process_library_folder(case_file_path, "cases")

# èŠå¤©ç•Œé¢
st.title("ç½‘å®‰æ™ºè„‘ AI")
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# æ¯æ¬¡åˆ‡æ¢ä¼šè¯æ—¶æ¸…é™¤å‰ç«¯èŠå¤©è®°å½•ï¼ˆä¸åˆ é™¤æ•°æ®åº“ä¸­çš„å†å²è®°å½•ï¼‰
if selected_session == "æ–°å»ºä¼šè¯":
    st.session_state.chat_history = []

# æ˜¾ç¤ºæœ€æ–°çš„å¯¹è¯
for message in st.session_state.chat_history:
    avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
    with st.chat_message(message["role"], avatar=avatar):
        # ç›´æ¥æ¸²æŸ“å·²å¤„ç†å†…å®¹ï¼ˆä¸å†æ›¿æ¢æ¢è¡Œç¬¦ï¼‰
        st.markdown(message["content"], unsafe_allow_html=False)

def get_response_content(response):
    """æ·±åº¦è§£æå“åº”å¯¹è±¡è·å–contentå†…å®¹"""
    try:
        # å°è¯•ç›´æ¥è®¿é—®contentå±æ€§
        if hasattr(response, 'content'):
            return response.content
        
        # å¤„ç†LangChainçš„AIMessageç±»å‹
        if hasattr(response, 'response_metadata'):
            return response.response_metadata.get('message', {}).get('content', '')
        
        # å¤„ç†å­—å…¸ç±»å‹çš„å“åº”
        if isinstance(response, dict):
            return response.get('content', '')
        
        # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„å“åº”
        if isinstance(response, str):
            content_match = re.search(r"content='(.*?)'", response, re.DOTALL)
            return content_match.group(1) if content_match else response
        
        return str(response)
    except Exception as e:
        print(f"å†…å®¹è§£æé”™è¯¯: {str(e)}")
        return "æ— æ³•è§£æå“åº”å†…å®¹"

# æ”¹è¿›åçš„é¢„å¤„ç†å‡½æ•°
def preprocess_response(text):
    """å¢å¼ºå‹å†…å®¹æ ¼å¼åŒ–"""
    # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾ï¼ˆå…³é”®ä¿®å¤ï¼‰
    text = re.sub(r'<[^>]+>', '', text)
    
    # å¤„ç†è½¬ä¹‰å­—ç¬¦ï¼ˆå°†åŒåæ–œæ è½¬å›å•åæ–œæ ï¼‰
    text = text.replace('\\\\', '\\')
    
    # ç»Ÿä¸€æ¢è¡Œç¬¦ä¸ºæ ‡å‡†çš„ \n
    text = re.sub(r'(\\n)|[\r\n]+', '\n', text)
    
    # è‡ªåŠ¨ä¿®å¤å¸¸è§Markdownæ ¼å¼
    text = re.sub(r'^(\s*)```', r'\1```', text, flags=re.MULTILINE)  # ä»£ç å—å¯¹é½
    text = re.sub(r'(?<!\\)(`{1,2})(?!`)', r'\\\1', text)  # ä¿æŠ¤å­¤ç«‹çš„åå¼•å·
    
    # å¤„ç†ä¸­æ–‡æ ‡ç‚¹ä¸ç‰¹æ®Šç¬¦å·
    replacements = {
        "â€˜": "'", "â€™": "'", "â€œ": '"', "â€": '"',
        "\\~": "~", "\\*": "*", "\\_": "_"
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    
    return text.strip()

    
user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ (è¾“å…¥ 'q' é€€å‡º)")
if user_input:
    if user_input.lower() == "q":
        st.write("ä¼šè¯å·²ç»“æŸ")
    else:
        predicted_label = classifier.predict(user_input)
        if predicted_label == 0:
            response = response_generator.generate_response_dailys(user_id, session_id, user_input)
        elif predicted_label == 1:
            response = response_generator.generate_response_nets(user_id, session_id, user_input, 
                                                                 net_faisser_indexer=faiss_indexer_net,
                                                                 rag_paragraphs_nets=net_paragraphs)
        elif predicted_label == 2:
            response = response_generator.generate_response_laws(user_id, session_id, user_input,
                                                                 laws_faiss_indexer=faiss_indexer_laws,
                                                                 cases_faiss_indexer=faiss_indexer_cases,
                                                                 rag_paragraphs_laws=law_paragraphs,
                                                                 rag_paragraphs_cases=case_paragraphs)
        elif predicted_label == 3:
            response = response_generator.analyze_case_with_law(user_id, session_id, user_input,
                                                                case_faiss_indexer=faiss_indexer_cases,
                                                                law_faiss_indexer=faiss_indexer_laws,
                                                                rag_paragraphs_cases=case_paragraphs,
                                                                rag_paragraphs_laws=law_paragraphs)

        raw_response = response  # ä¿ç•™åŸå§‹å“åº”å¯¹è±¡
        
        # æ·±åº¦è§£æå†…å®¹
        raw_content = get_response_content(raw_response)
        
        # äºŒæ¬¡éªŒè¯ï¼ˆç¡®ä¿æå–æ­£ç¡®ï¼‰
        if "content='" in str(raw_response):
            content_match = re.search(r"content='(.*?)'", str(raw_response), re.DOTALL)
            fallback_content = content_match.group(1) if content_match else raw_content
            raw_content = fallback_content
        
        # ç»Ÿä¸€é¢„å¤„ç†
        processed_content = preprocess_response(raw_content)
        
        # æ›´æ–°èŠå¤©å†å²è®°å½•
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        st.session_state.chat_history.append({"role": "assistant", "content": processed_content})

        # æ˜¾ç¤ºæœ€æ–°æ¶ˆæ¯
        with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
            st.markdown(user_input)
        with st.chat_message("assistant", avatar='ğŸ¤–'):
            st.markdown(processed_content, unsafe_allow_html=False)